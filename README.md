# Firecrawl Content Migrator

A powerful web scraping and content migration tool built with Next.js, TypeScript, and the Firecrawl API. Extract structured data from any website and export it in formats ready for your CMS, database, or data pipeline.

üîó **Repository**: [github.com/mendableai/firecrawl-migrator](https://github.com/mendableai/firecrawl-migrator)

## Features

- **Smart Website Mapping**: Analyze and visualize website structure with an interactive tree view
- **Selective URL Scraping**: Choose exactly which pages to scrape with intelligent filtering
- **Custom Data Schemas**: Define custom fields to extract specific content (title, date, author, etc.)
- **Batch Processing**: Efficiently scrape multiple URLs in a single operation
- **Multiple Export Formats**: Export to CSV, JSON, or custom formats
- **Real-time Progress**: Monitor scraping progress with live updates
- **Cost-Effective**: Uses Map + Batch Scrape strategy to minimize API credits

## Use Cases

- **Blog Migration**: Extract posts, metadata, and content from any blog platform
- **E-commerce Data**: Scrape product information, prices, and descriptions
- **News Archives**: Collect articles with dates, authors, and categories
- **Documentation Sites**: Extract technical documentation with proper structure
- **Content Audits**: Analyze and export existing website content

## Prerequisites

- Node.js 18+ and npm
- Firecrawl API key (get one at [firecrawl.dev](https://firecrawl.dev))

## Quick Start

### 1. Clone the repository
```bash
git clone https://github.com/mendableai/firecrawl-migrator.git
cd firecrawl-migrator
```

### 2. Install dependencies
```bash
npm install
```

### 3. Get your Firecrawl API key
- Sign up at [firecrawl.dev](https://firecrawl.dev)
- Navigate to your dashboard
- Copy your API key

### 4. Configure environment variables
Create a `.env.local` file in the root directory:
```bash
touch .env.local
```

Add your Firecrawl API key to the file:
```env
FIRECRAWL_API_KEY=fc-YOUR_ACTUAL_API_KEY_HERE
```

‚ö†Ô∏è **Important**: Replace `fc-YOUR_ACTUAL_API_KEY_HERE` with your actual API key from Firecrawl dashboard.

### 5. Run the development server
```bash
npm run dev
```

### 6. Open the application
Visit [http://localhost:3000](http://localhost:3000) in your browser

You should see the Firecrawl Content Migrator interface. If you see an API key error, double-check your `.env.local` file.

## How It Works

### Step 1: Map Website Structure
Enter a URL to analyze the website's structure. The mapping operation discovers all available pages and organizes them in a hierarchical tree view. (Uses 2 API credits)

### Step 2: Select Content
Browse the interactive tree and select which pages to scrape. Use the built-in filters to:
- Select all pages in a directory
- Filter by URL patterns
- Exclude categories, tags, or pagination

### Step 3: Define Schema
Configure what data to extract from each page:
- **Default fields**: title, date, content
- **Add custom fields**: author, category, price, tags, etc.
- **Auto-detection**: The tool can analyze pages and suggest fields

### Step 4: Extract & Export
Start the batch scraping process to extract structured data from all selected pages. Export results as:
- CSV for spreadsheets and databases
- JSON for APIs and applications
- Custom formats for specific CMS platforms

## API Endpoints

### POST /api/map
Maps website structure and returns discovered URLs
```json
{
  "url": "https://example.com/blog",
  "limit": 200
}
```

### POST /api/crawl
Extracts content from selected URLs with schema
```json
{
  "url": "https://example.com",
  "selectedUrls": ["url1", "url2"],
  "schema": {
    "properties": {
      "title": { "type": "string" },
      "date": { "type": "string" },
      "content": { "type": "string" }
    }
  }
}
```

## Advanced Features

### Smart URL Detection
The migrator intelligently identifies content patterns:
- Blog posts vs category pages
- Product pages vs listings
- Documentation vs navigation

### Schema Auto-Detection
Analyzes page content to suggest relevant fields:
- Detects prices, dates, authors
- Identifies metadata patterns
- Recognizes common content structures

### Efficient Credit Usage
Optimized strategy minimizes API costs:
- Map operation: 2 credits (one-time)
- Batch scrape: 1 credit per URL
- Example: 100 blog posts = 102 total credits

## Project Structure

```
firecrawl-content-migrator/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crawl/        # Content extraction endpoint
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ map/          # Website mapping endpoint
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx          # Main application UI
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ layout.tsx        # App layout
‚îÇ   ‚îî‚îÄ‚îÄ components/
‚îÇ       ‚îú‚îÄ‚îÄ layout/           # Layout components
‚îÇ       ‚îî‚îÄ‚îÄ ui/               # Reusable UI components
‚îú‚îÄ‚îÄ public/                   # Static assets
‚îî‚îÄ‚îÄ package.json             # Dependencies
```

## Troubleshooting

### "Firecrawl API key not configured"
- Ensure `.env.local` exists with valid API key
- Restart dev server after adding key

### "Failed to map website"
- Verify URL is accessible
- Check API key has credits
- Try a different URL path

### Batch scraping fails
- App automatically falls back to individual scraping
- Check browser console for errors
- Verify selected URLs are valid

## Development

### Build for production
```bash
npm run build
npm start
```

### Run linting
```bash
npm run lint
```

## Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new features
4. Submit a pull request

## License

MIT License - see LICENSE file for details

## Support

- Create an issue on GitHub
- Check [Firecrawl docs](https://docs.firecrawl.dev)
- Join [Firecrawl Discord](https://discord.gg/firecrawl)